{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "collapsed_sections": [
        "X5wd50m0DE9T",
        "VSeIhnxlDJ2d",
        "MGbuGvxfEgTT",
        "7emgm9q7K1oY",
        "kjoS15avRTyD"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "uTFzoTAbeL5t"
      },
      "cell_type": "markdown",
      "source": [
        "# **Credit Card Fraud**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Libraries**"
      ],
      "metadata": {
        "id": "1bRlpXfjDBc3"
      }
    },
    {
      "metadata": {
        "id": "0ensrchgdNnW"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tfp.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rcANmCk2vJG0"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfpl = tfp.layers\n",
        "tfd = tfp.distributions\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sp--65fYCUYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading Data**"
      ],
      "metadata": {
        "id": "X5wd50m0DE9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4kbYYCxdCkfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path= '/content/drive/MyDrive/Data/creditcard.csv'\n",
        "\n",
        "raw_data = pd.read_csv(file_path)\n",
        "data, data_test = train_test_split(raw_data, test_size=0.25)"
      ],
      "metadata": {
        "id": "hkqRHuu2ClKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploratory Analysis**"
      ],
      "metadata": {
        "id": "VSeIhnxlDJ2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.loc[:,\"Time\"] = data[\"Time\"].apply(lambda x : x / 3600 % 24)\n",
        "data.loc[:,'Amount'] = np.log(data['Amount']+1)\n",
        "\n",
        "data_test.loc[:,\"Time\"] = data_test[\"Time\"].apply(lambda x : x / 3600 % 24)\n",
        "data_test.loc[:,'Amount'] = np.log(data_test['Amount']+1)\n",
        "# data = data.drop(['Amount'], axis = 1)\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "VAr6vNHIC6Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def tsne_plot(X, Y, filename=\"tsne.png\"):\n",
        "    tsne = TSNE(n_components=2, random_state=0)\n",
        "    X_2d = tsne.fit_transform(X)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    target_ids = range(len(set(Y)))\n",
        "    colors = 'r', 'g'  # Adjust colors as needed\n",
        "    for i, c, label in zip(target_ids, colors, set(Y)):\n",
        "        plt.scatter(X_2d[Y == i, 0], X_2d[Y == i, 1], c=c, label=label)\n",
        "    plt.legend()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "non_fraud = data[data['Class'] == 0].sample(1000)\n",
        "fraud = data[data['Class'] == 1]\n",
        "\n",
        "df = pd.concat([non_fraud, fraud]).sample(frac=1).reset_index(drop=True)\n",
        "X = df.drop(['Class'], axis = 1).values\n",
        "Y = df[\"Class\"].values\n",
        "\n",
        "tsne_plot(X, Y, \"original.png\")"
      ],
      "metadata": {
        "id": "BPV8ZrhKC-QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train a VAE**"
      ],
      "metadata": {
        "id": "MGbuGvxfEgTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train a VAE with 100k in-sample non-fraud transactions. As this is for exploratory and illustration purpose, the hidden layer design was by trial and error.\n",
        "\n",
        "The prior for the latent variables was set to be a random unit multivariate normal vector of the latent dimention. The latent dimension is set to 2 so that it give some intuitive illustrations as you will see soon.\n",
        "\n",
        "The output of the encoder, the latent distribution parameters, was deliberated chosen to be multiviarate normal with non-zero covariance because I noticed it had subsequent impact on the separation of normal transactions from fraud transactions, suggesting that the covariance of fraud transactions may have patterns. As a result, there are 5 distribution parameters to be learnt (2 mean values + 3 covariance values from the lower triangle of the 2-buy-2 covariance matrix)\n",
        "\n",
        "The output of the decoder, the data distribution parameters, follow feature-independent normal distributions. This choice is important. Most of the examples I could find online were applied to binary images such as the MNIST dataset where the output would follow independent bernoulli distributions. Here the data are real-valued and generally follow normal distributions, hence it only makes sense to model the output with normal distributions or alike. Another important implication of having the right distribution is that it will give the corresponding log probability loss during training. It would not make sense to train a real-valued normal distribution using binary cross entropy, for example."
      ],
      "metadata": {
        "id": "13VmpzqOEeen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "tfd = tfp.distributions\n",
        "\n",
        "\n",
        "def dense_layers(sizes):\n",
        "    return tfk.Sequential([tfkl.Dense(size, activation=tf.nn.leaky_relu) for size in sizes])\n",
        "\n",
        "original_dim = X.shape[1]\n",
        "input_shape = X[0].shape\n",
        "intermediary_dims = [20, 10, 8]\n",
        "latent_dim = 2\n",
        "batch_size = 128\n",
        "max_epochs = 1000\n",
        "\n",
        "# prior = tfd.Independent(tfd.Normal(loc=tf.zeros(latent_dim), scale=1),\n",
        "#                         reinterpreted_batch_ndims=1)\n",
        "\n",
        "prior = tfd.MultivariateNormalDiag(\n",
        "        loc=tf.zeros([latent_dim]))\n",
        "        #scale_identity_multiplier=1.0)\n",
        "\n",
        "encoder = tfk.Sequential([\n",
        "    tfkl.InputLayer(input_shape=input_shape, name='encoder_input'),\n",
        "    dense_layers(intermediary_dims),\n",
        "    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None),\n",
        "    tfpl.MultivariateNormalTriL(latent_dim,\n",
        "                           activity_regularizer=tfpl.KLDivergenceRegularizer(prior)),\n",
        "], name='encoder')\n",
        "\n",
        "encoder.summary()\n",
        "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "decoder = tfk.Sequential([\n",
        "    tfkl.InputLayer(input_shape=[latent_dim]),\n",
        "    dense_layers(reversed(intermediary_dims)),\n",
        "    tfkl.Dense(tfpl.IndependentNormal.params_size(original_dim), activation=None),\n",
        "    tfpl.IndependentNormal(original_dim),\n",
        "], name='decoder')\n",
        "\n",
        "decoder.summary()\n",
        "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "vae = tfk.Model(inputs=encoder.inputs,\n",
        "                outputs=decoder(encoder.outputs[0]),\n",
        "                name='vae_mlp')\n",
        "\n",
        "negloglik = lambda x, rv_x: -rv_x.log_prob(x)\n",
        "\n",
        "vae.compile(optimizer=tf.keras.optimizers.Nadam(),\n",
        "            loss=negloglik)\n",
        "\n",
        "vae.summary()\n",
        "plot_model(vae,\n",
        "           to_file='vae_mlp.png',\n",
        "           show_shapes=True)"
      ],
      "metadata": {
        "id": "E-r8XwdODXx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.drop([\"Class\"], axis=1)\n",
        "y = data[\"Class\"].values\n",
        "\n",
        "x_norm, x_fraud = x.values[y == 0], x.values[y == 1]\n",
        "\n",
        "x_norm_sample = x_norm[np.random.randint(x_norm.shape[0], size=100000), :]\n",
        "x_norm_train_sample, x_norm_val_sample = train_test_split(x_norm_sample, test_size=0.2)"
      ],
      "metadata": {
        "id": "8gOY895QElot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train = tf.data.Dataset.from_tensor_slices((x_norm_train_sample, x_norm_train_sample)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))\n",
        "tf_val = tf.data.Dataset.from_tensor_slices((x_norm_val_sample, x_norm_val_sample)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))"
      ],
      "metadata": {
        "id": "NmMHSd8oFfTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='bestmodel.h5', verbose=0, save_best_only=True)\n",
        "earlystopper = EarlyStopping(monitor='val_loss', mode='min', min_delta=0.005, patience=20, verbose=0, restore_best_weights=True)\n",
        "\n",
        "hist = vae.fit(tf_train,\n",
        "               epochs=max_epochs,\n",
        "               shuffle=True,\n",
        "               verbose=0,\n",
        "               validation_data=tf_val,\n",
        "               callbacks=[checkpointer, earlystopper])\n",
        "\n"
      ],
      "metadata": {
        "id": "fFPADkK4FhQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The training stops when the validation losses fail to decrease for 20 consecutive epochs.**"
      ],
      "metadata": {
        "id": "CzF8P-pZMw0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot Keras training history\n",
        "def plot_loss(hist):\n",
        "    plt.plot(hist.history['loss'])\n",
        "    plt.plot(hist.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.yscale('log',base=10)\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(hist)\n"
      ],
      "metadata": {
        "id": "spK7FbpDFj0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruct_samples_n = 100\n",
        "\n",
        "def reconstruction_log_prob(eval_samples, reconstruct_samples_n):\n",
        "    encoder_out = encoder(eval_samples)\n",
        "    encoder_samples = encoder_out.sample(reconstruct_samples_n)\n",
        "    return np.mean(decoder(encoder_samples).log_prob(eval_samples), axis=0)"
      ],
      "metadata": {
        "id": "GuEZz5fUMUEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_x_mean = encoder(X).mean()\n",
        "plt.scatter(latent_x_mean[:, 0], latent_x_mean[:, 1], c=Y, cmap='RdYlGn_r', s=2)\n",
        "plt.title('latent means')\n",
        "plt.ylabel('mean[1]')\n",
        "plt.xlabel('mean[0]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFUYK3n7M47T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_x_std = encoder(X).stddev()\n",
        "plt.scatter(latent_x_std[:, 0], latent_x_std[:, 1], c=Y, cmap='RdYlGn_r', s=2)\n",
        "plt.title('latent standard deviations')\n",
        "plt.ylabel('stddev[1]')\n",
        "plt.xlabel('stddev[0]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "acFd-UQeM7Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_x = encoder(X).sample()\n",
        "plt.scatter(latent_x[:, 0], latent_x[:, 1], c=Y, cmap='RdYlGn_r', s=2)\n",
        "plt.title('latent vector samples')\n",
        "plt.ylabel('z[1]')\n",
        "plt.xlabel('z[0]')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4aZjwi1bNQ8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azhcgZdmLZmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_layers(sizes):\n",
        "    return tfk.Sequential([tfkl.Dense(size, activation=tf.nn.leaky_relu) for size in sizes])\n",
        "\n",
        "# Model parameters\n",
        "original_dim = X.shape[1]\n",
        "input_shape = X[0].shape\n",
        "intermediary_dims = [20, 10, 8]\n",
        "latent_dim = 2\n",
        "batch_size = 128\n",
        "max_epochs = 1000\n",
        "\n",
        "# Prior distribution\n",
        "prior = tfd.MultivariateNormalDiag(\n",
        "    loc=tf.zeros([latent_dim]),\n",
        "    scale_diag=tf.ones([latent_dim])\n",
        ")\n",
        "\n",
        "# Encoder model\n",
        "encoder = tfk.Sequential([\n",
        "    tfkl.InputLayer(input_shape=input_shape, name='encoder_input'),\n",
        "    dense_layers(intermediary_dims),\n",
        "    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None),\n",
        "    tfpl.MultivariateNormalTriL(latent_dim, activity_regularizer=tfpl.KLDivergenceRegularizer(prior)),\n",
        "], name='encoder')\n",
        "\n",
        "encoder.summary()\n",
        "plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
        "\n",
        "# Decoder model\n",
        "decoder = tfk.Sequential([\n",
        "    tfkl.InputLayer(input_shape=[latent_dim], name='decoder_input'),\n",
        "    dense_layers(reversed(intermediary_dims)),\n",
        "    tfkl.Dense(tfpl.IndependentNormal.params_size(original_dim), activation=None),\n",
        "    tfpl.IndependentNormal(original_dim),\n",
        "], name='decoder')\n",
        "\n",
        "decoder.summary()\n",
        "plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
        "\n",
        "# VAE model\n",
        "vae = tfk.Model(inputs=encoder.inputs, outputs=decoder(encoder.outputs[0]), name='vae_mlp')\n",
        "\n",
        "negloglik = lambda x, rv_x: -rv_x.log_prob(x)\n",
        "\n",
        "vae.compile(optimizer=tf.keras.optimizers.Nadam(), loss=negloglik)\n",
        "\n",
        "vae.summary()\n",
        "plot_model(vae, to_file='vae_mlp.png', show_shapes=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "q0ZgVJoCLcWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruction_log_prob(eval_samples, reconstruct_samples_n):\n",
        "    # Convert eval_samples to tf.float32 to match model dtype\n",
        "    eval_samples = tf.convert_to_tensor(eval_samples, dtype=tf.float32)\n",
        "\n",
        "    # Get encoder output\n",
        "    encoder_out = encoder(eval_samples)\n",
        "\n",
        "    # Sample from the encoder's distribution\n",
        "    encoder_samples = encoder_out.sample(reconstruct_samples_n)\n",
        "\n",
        "    # Reshape encoder_samples to match the decoder's expected input shape\n",
        "    encoder_samples_reshaped = tf.reshape(encoder_samples, [-1, latent_dim])  # Flatten along batch and samples\n",
        "\n",
        "    # Decode the reshaped samples\n",
        "    decoder_out = decoder(encoder_samples_reshaped)\n",
        "\n",
        "    # Reshape eval_samples to match decoder_out for log_prob calculation\n",
        "    eval_samples_reshaped = tf.repeat(eval_samples, repeats=reconstruct_samples_n, axis=0) # Repeat each sample to match the number of reconstructed samples\n",
        "\n",
        "    # Compute log probability for each sample individually, keeping the sample dimension\n",
        "    log_prob = -decoder_out.log_prob(eval_samples_reshaped)\n",
        "\n",
        "    # Calculate the mean log probability for each original sample across the reconstructed samples\n",
        "    log_prob = tf.reduce_mean(tf.reshape(log_prob, [tf.shape(eval_samples)[0], reconstruct_samples_n]), axis=1)\n",
        "\n",
        "    return log_prob.numpy()  # Convert TensorFlow tensor to NumPy array\n",
        "\n",
        "# Assuming `X` and `reconstruct_samples_n` are defined\n",
        "reconstruct_samples_n = 100  # Example value, adjust as needed\n",
        "x_log_prob = reconstruction_log_prob(X, reconstruct_samples_n)\n"
      ],
      "metadata": {
        "id": "ugtucy0lLcZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[x_log_prob[Y == 0][0:10], x_log_prob[Y == 1][0:10]]"
      ],
      "metadata": {
        "id": "qWZjkCq2QRtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plotting the histogram\n",
        "plt.hist([x_log_prob[Y == 0][0:10], x_log_prob[Y == 1][0:10]], bins=60, label=['Class 0', 'Class 1'])\n",
        "plt.title('Reconstruction Log Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel(\"log p(x|x')\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G327UfNJPnWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's take the negative reconstruction log probability, and draw a ROC curve across the range to see how it would perform if we were to build a threshold-based fraud detector on it."
      ],
      "metadata": {
        "id": "OxkNfh-DRCNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, thresh = roc_curve(Y, x_log_prob)\n",
        "auc = roc_auc_score(Y, x_log_prob)\n",
        "\n",
        "plt.plot(fpr,tpr,label=\"linear in-sample, auc=\"+str(auc))\n",
        "plt.title('VAE roc curve - training')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "990SbjXuREYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **VAE vs SVM**"
      ],
      "metadata": {
        "id": "7emgm9q7K1oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(gamma='scale')\n",
        "clf.fit(X, Y)\n",
        "auc = roc_auc_score(Y, clf.predict(X))\n",
        "\n",
        "plt.plot(fpr,tpr,label=\"linear in-sample, auc=\"+str(auc))\n",
        "plt.title('SVM roc curve - training')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GyGN2Vk9Q-HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation on Test**"
      ],
      "metadata": {
        "id": "kjoS15avRTyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_log_prob = reconstruction_log_prob(data_test.drop(['Class'], axis = 1).values, reconstruct_samples_n)\n",
        "test_y = data_test[\"Class\"].values\n",
        "\n",
        "fpr, tpr, thresh = roc_curve(test_y, x_test_log_prob)\n",
        "auc = roc_auc_score(test_y, x_test_log_prob)\n",
        "\n",
        "plt.plot(fpr,tpr,label=\"linear in-sample, auc=\"+str(auc))\n",
        "plt.title('VAE roc curve - test')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rzNoDfdzK4ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc = roc_auc_score(test_y, clf.predict(data_test.drop(['Class'], axis = 1).values))\n",
        "\n",
        "plt.plot(fpr,tpr,label=\"linear in-sample, auc=\"+str(auc))\n",
        "plt.title('SVM roc curve - test')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2l6umROyRWjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The VAE hidden layer design has room for improvement.\n",
        "- Hyperparameter tuning can be added."
      ],
      "metadata": {
        "id": "G3Q6CoYJRhOr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IWMbqG14Rd2g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}